# -*- coding: utf-8 -*-
"""NN_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y3ZdrZ8VCWaAk372Dy785IKo4nzXxt2T
"""

from google.colab import drive
drive.mount('/content/drive')

import csv

# opening the CSV file
with open('/content/drive/MyDrive/BTP/train.csv', mode ='r')as file:

  # reading the CSV file
  csvFile = csv.reader(file)

  # displaying the contents of the CSV file
  dic = {}
  i = 0
  for lines in csvFile:
        print(lines)
        print(lines[2])
        if i == 2:
          break
        i+=1

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/BTP/train.csv')

dic = {}


for i,row in df.iterrows():
  if row["cuisine"] not in dic:
    dic[row["cuisine"]] = 1
  else:
    dic[row["cuisine"]] += 1

from ast import literal_eval

# Data Cleaning: Convert ingredients from string representation of lists to actual lists
df['ingredients'] = df['ingredients'].apply(literal_eval)

"""word2vec"""

from gensim.models import Word2Vec

# Flatten the list of ingredients
all_ingredients = [ingredient for ingredients_list in df['ingredients'] for ingredient in ingredients_list]

# Train the Word2Vec model
# Since the dataset might not be very large, we'll use a smaller vector size and window
model = Word2Vec([all_ingredients], vector_size=100, window=5, min_count=1, workers=4)

# Vectorize the ingredients per recipe
# We'll average the vectors of the ingredients for each recipe
def vectorize_ingredients(ingredients_list):
    vector = sum([model.wv[ingredient] for ingredient in ingredients_list if ingredient in model.wv]) / len(ingredients_list)
    return vector

df['ingredients_vector'] = df['ingredients'].apply(vectorize_ingredients)

# Display the updated dataframe
df.head()

"""bert"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

# Load pre-trained model tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertModel.from_pretrained('bert-base-uncased')

def bert_vectorize_ingredients(ingredients_list):
    # Join the ingredients into a single string
    ingredients_text = ' '.join(ingredients_list)
    # Tokenize the text
    inputs = tokenizer(ingredients_text, return_tensors='tf', truncation=True, max_length=512)
    # Generate embeddings
    outputs = model(**inputs)
    # Retrieve the embeddings (average of the last hidden state)
    embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1)
    return embeddings.numpy()

# Vectorize the ingredients using BERT
df['ingredients_bert_vector'] = df['ingredients'].apply(lambda x: bert_vectorize_ingredients(x)[0])

"""label encoder"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import numpy as np

# Convert the vectors to a numpy array
vectors = np.array(df['ingredients_vector'].tolist())

# Label Encoding
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(df['cuisine'])

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(vectors, labels, test_size=0.2, random_state=42)

# Checking the shape of the training and testing data
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""model"""

import tensorflow as tf
import keras

# Number of unique cuisines (classes)
num_classes = len(label_encoder.classes_)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512, activation='relu', input_shape=(100,), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])

"""testing"""

print(df.shape)
print(df.iloc[:, 0])
print(df.iloc[:, 1])
print(df.iloc[:, 2])
print(df.iloc[:, 3])

in_dic = {}
f=0

for i,row in df.iterrows():
  for ing in str(row["ingredients"][1:-1]).split(","):
    if "'" in ing:
      ing = ing[ing.index("'")+1:]
    if "'" in ing:
      ing = ing[:ing.index("'")]

    if str(ing) not in in_dic.keys():
      in_dic[str(ing)] = 1
    else:
      f+=1
      in_dic[str(ing)] += 1

temp_df = df.copy()
for i in dic.keys():
  if dic[i] < dic['southern_us']:
    df.drop(temp_df.index[(temp_df["cuisine"] == i)],inplace=True)

size = df.shape[0]
train = {"southern_us" : [] , "italian" : [] , "mexican" : []}
for i,row in df.iloc[:int(0.80*size)].iterrows():
  temp = []
  for j in str(row["ingredients"][1:-1]).split(","):
    ing = j.replace("'" , "")
    temp.append(ing)
  train[row["cuisine"]].append(temp)

test = {"southern_us" : [] , "italian" : [] , "mexican" : []}
for i,row in df.iloc[int(0.80*size):].iterrows():
  temp = []
  for j in str(row["ingredients"][1:-1]).split(","):
    ing = j.replace("'" , "")
    temp.append(ing)
  test[row["cuisine"]].append(temp)

size = df.shape[0]
full = {"southern_us" : [] , "italian" : [] , "mexican" : []}
for i,row in df.iterrows():
  temp = []
  for j in str(row["ingredients"][1:-1]).split(","):
    ing = j.replace("'" , "")
    temp.append(ing)
  full[row["cuisine"]].append(temp)

print(len(train["italian"]))
print(len(test["italian"]))
print(len(train["southern_us"]))
print(len(test["southern_us"]))
print(len(train["mexican"]))
print(len(test["mexican"]))

ind = {}
f = 1
for i in train.keys():
  for j in train[i]:
    for ing in j:
      if ing not in ind:
        ind[ing] = f
        f+=1
print(len(ind))

for i in test.keys():
  for j in test[i]:
    for ing in j:
      if ing not in ind:
        ind[ing] = f
        f+=1
print(len(ind))

ind = 1
index = []
cuisine = []
ingredients = []
for i in full.keys():
  for j in full[i]:
    index.append(ind)
    ind+=1
    cuisine.append(i)
    ingredients.append(j)

train_df = pd.DataFrame({'id':index,
                   'cuisine':cuisine,
                   'ingredients':ingredients})
train_df = train_df.sample(frac = 1)
print(train_df.head(10))

new = []
for s in train_df['ingredients']:
    s = ' '.join(s)
    new.append(s)
train_df['ing'] = new
train_df['cuisine'].value_counts()

import re
import nltk
nltk.download("popular")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
ps = PorterStemmer()


l=[]
for s in train_df['ing']:

    #Remove punctuations
    s=re.sub(r'[^\w\s]','',s)

    #Remove Digits
    s=re.sub(r"(\d)", "", s)

    #Remove content inside paranthesis
    s=re.sub(r'\([^)]*\)', '', s)

    #Remove Brand Name
    s=re.sub(u'\w*\u2122', '', s)

    #Convert to lowercase
    s=s.lower()

    #Remove Stop Words
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(s)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    filtered_sentence = []
    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)
    s=' '.join(filtered_sentence)

    #Remove low-content adjectives


    #Porter Stemmer Algorithm
    words = word_tokenize(s)
    word_ps=[]
    for w in words:
       word_ps.append(ps.stem(w))
    s=' '.join(word_ps)

    l.append(s)
train_df['ing_mod']=l
print(train_df.head(10))

train_df.head(5)

import numpy as np

train_recipe = []
train_label = []
for i in range(len(train_df['ing_mod'])):
  if train_df['cuisine'][i] != 2:
    train_recipe.append(train_df['ing_mod'][i].split(" "))
    train_label.append(train_df["cuisine"][i])
print(len(train_recipe))
print(len(train_label))
print(train_recipe[0][0])

"""embeddings"""

import gensim
from gensim.models import Word2Vec, KeyedVectors

model = Word2Vec(window=5, min_count=2, workers=4, sg=0)

model.build_vocab(train_recipe, progress_per=1000)
model.train(train_recipe, total_examples=model.corpus_count, epochs=model.epochs)

train_ing = []
for i in range(len(train_df['ingredients'])):
  for j in range(len(train_df['ingredients'][i])):
    train_ing.append(train_df['ingredients'][i][j])
print(len(train_ing))

train_ing.index('low-fat mayonnaise')

# train_df['cuisine'].value_counts()

# from sklearn import preprocessing
# le = preprocessing.LabelEncoder()
# le.fit(train_df['cuisine'])
# train_df['cuisine']=le.transform(train_df['cuisine'])

# train_df['cuisine'].value_counts()

x = []
y = []
for i in range(len(train_recipe)):
  t = []
  for j in range(len(train_recipe[i])):
    t.append(model.wv[train_recipe[i][j]])
  x.append(t)
  y.append(train_label)

import tensorflow as tf
import keras

model = tf.keras.Sequential([
    keras.layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),
    keras.layers.Dense(units=256, activation='relu'),
    keras.layers.Dense(units=192, activation='relu'),
    keras.layers.Dense(units=128, activation='relu'),
    keras.layers.Dense(units=10, activation='softmax')
])