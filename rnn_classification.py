# -*- coding: utf-8 -*-
"""RNN_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mX2_SzToxuZGGrMy0ND1k3U9lGnwHjcT
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU  # Choose based on requirement
from scipy.sparse import csr_matrix
from tensorflow.keras.utils import to_categorical

# Load the dataset
recipes_data = pd.read_csv('/content/drive/MyDrive/BTP/train.csv')

# Preprocessing steps

# Combine all ingredients into a single string per recipe
recipes_data['all_ingredients'] = recipes_data['ingredients'].apply(lambda x: ' '.join(eval(x)))

# TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=1000)  # You can adjust the number of features
tfidf_features = tfidf.fit_transform(recipes_data['all_ingredients'])

# Encode the labels (cuisines)
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(recipes_data['cuisine'])

# Convert labels to categorical
categorical_labels = to_categorical(encoded_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, categorical_labels, test_size=0.2, random_state=42)

# Model Building

# Parameters
input_dim = X_train.shape[1]  # Number of features from TF-IDF
rnn_units = 128  # RNN units
print(label_encoder.classes_)
# Build the RNN model
model = Sequential()
model.add(Dense(100, activation='relu', input_dim=input_dim))  # Adjust the number of units
model.add(Dense(rnn_units, activation='relu'))
model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Training the model
history = model.fit(X_train.toarray(), y_train, epochs=5, batch_size=128, validation_data=(X_test.toarray(), y_test))

# Evaluate the model's performance using the test set.

print(label_encoder.classes_)

import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load the dataset
recipes_data = pd.read_csv('/content/drive/MyDrive/BTP/train.csv')

# Preprocessing steps

# Combine all ingredients into a single string per recipe
recipes_data['all_ingredients'] = recipes_data['ingredients'].apply(lambda x: ' '.join(eval(x)))

# Tokenize the ingredients
tokenizer = Tokenizer()
tokenizer.fit_on_texts(recipes_data['all_ingredients'])
sequences = tokenizer.texts_to_sequences(recipes_data['all_ingredients'])

# Pad sequences to ensure uniform length
max_seq_length = max(len(seq) for seq in sequences)
padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')

# Word2Vec Model
# Either train a new model or load a pre-trained model
# For training a new model:
words = [ingredient.split() for ingredient in recipes_data['all_ingredients']]
word2vec_model = Word2Vec(words, vector_size=10, window=3, min_count=1, workers=4)
word2vec_model.train(words, total_examples=len(words), epochs=10)

# Create an embedding matrix
embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 10))  # Assuming 10 dimensions in Word2Vec
for word, i in tokenizer.word_index.items():
    if word in word2vec_model.wv:
        embedding_matrix[i] = word2vec_model.wv[word]

# Encode the labels (cuisines)
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(recipes_data['cuisine'])

# Convert labels to categorical
categorical_labels = to_categorical(encoded_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, categorical_labels, test_size=0.2, random_state=42)

# Model Building

# Build the RNN model
model = Sequential()
model.add(Embedding(len(tokenizer.word_index) + 1, 10, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))
model.add(LSTM(128))
model.add(Dense(len(label_encoder.classes_), activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Training the model
history = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model's performance using the test set.

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.utils import to_categorical

# Load the dataset
recipes_data = pd.read_csv('/content/drive/MyDrive/BTP/train.csv')

# Preprocessing steps

# Combine all ingredients into a single string per recipe
recipes_data['all_ingredients'] = recipes_data['ingredients'].apply(lambda x: ' '.join(eval(x)))

# TF-IDF Vectorization
tfidf = TfidfVectorizer()  # Adjust the number of features as needed
tfidf_features = tfidf.fit_transform(recipes_data['all_ingredients'])

# Encode the labels (cuisines)
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(recipes_data['cuisine'])

# Convert labels to categorical
categorical_labels = to_categorical(encoded_labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, categorical_labels, test_size=0.2, random_state=42)

# Model Building

# Parameters
input_dim = X_train.shape[1]  # Number of features from TF-IDF
lstm_units = 128  # LSTM units

# Building the LSTM model
model = Sequential()
model.add(LSTM(units=lstm_units, input_shape=(input_dim, 1)))
model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Reshape input data to be 3D (samples, timesteps, features) for LSTM
X_train_reshaped = X_train.toarray().reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_reshaped = X_test.toarray().reshape((X_test.shape[0], X_test.shape[1], 1))

# Training the model
history = model.fit(X_train_reshaped, y_train, epochs=5, batch_size=128, validation_data=(X_test_reshaped, y_test))

# The accuracy of LSTM and recurrent neural networks in this task is very less