# -*- coding: utf-8 -*-
"""CuisineFusion_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SQiTjgivb-9xMiIT4GpPEtLZ4LT3D1me
"""

from google.colab import drive
drive.mount('/content/drive')

import json
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
import pandas as pd
import matplotlib.pyplot as plt

ps = PorterStemmer()
file = r'/content/drive/MyDrive/Cuisine Fusion/train.json'
recipes = pd.read_json("/content/drive/MyDrive/Cuisine Fusion/train.json")
with open(file) as train_file:
    dict_train = json.load(train_file)

print(dict_train[0])

len(dict_train)

recipes["num_ingredients"] = recipes.ingredients.map(lambda x:len(x))
print(recipes.num_ingredients.describe())
sort_biggest = recipes.sort_values(by=["num_ingredients"], ascending=False)

import seaborn as sns
plt.figure(figsize=(8,6))
plt.title("Histogram of the Number of Ingredients Across All Recipes")
sns.histplot(recipes.num_ingredients)

avg_ingr_count = recipes.groupby("cuisine").num_ingredients.mean()
avg_ingr_count = avg_ingr_count.sort_values()
plt.figure(figsize=(8,6))
plt.title("Average Number of Ingredients for Each Cuisine")
sns.barplot(x=avg_ingr_count,y=avg_ingr_count.index)

cusine_counts = recipes.cuisine.value_counts()
plt.figure(figsize=(8,6))
plt.title("Number of Recipes for Each Cuisine")
sns.barplot(x=cusine_counts, y=cusine_counts.index)

top_5 = ["italian" , "mexican" , "southern_us" , "indian" , "chinese"]
top_ing = {"italian":{} , "mexican":{} , "southern_us":{} , "indian":{} , "chinese":{}}
for i in dict_train:
  if i["cuisine"] in top_5:
    for j in i["ingredients"]:
      if j not in top_ing[i["cuisine"]]:
        top_ing[i["cuisine"]][j] = 1
      else:
        top_ing[i["cuisine"]][j] += 1

for i in top_ing.keys():
  top_ing[i] = dict(sorted(top_ing[i].items(), key=lambda item: item[1] , reverse=True))

import matplotlib.pyplot as plt
for i in top_ing.keys():
  curr_ing = []
  ing_freq = []
  for j in list(top_ing[i].keys())[:5]:
    curr_ing.append(j)
    ing_freq.append(top_ing[i][j])


  fig = plt.figure(figsize = (10, 3))
  plt.title(i)
  plt.bar(curr_ing , ing_freq , width=0.4)
  plt.show()

id_ = []
cuisine = []
ingredients = []
for i in range(len(dict_train)):
    id_.append(dict_train[i]['id'])
    cuisine.append(dict_train[i]['cuisine'])
    ingredients.append(dict_train[i]['ingredients'])

import pandas as pd
df = pd.DataFrame({'id':id_,
                   'cuisine':cuisine,
                   'ingredients':ingredients})
print(df.head(5))

from wordcloud import WordCloud
x= df['cuisine'].values

plt.subplots(figsize = (8,8))

wordcloud = WordCloud (
                    background_color = 'black',
                    width = 712,
                    height = 384).generate(' '.join(x))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
from matplotlib.pyplot import figure

bag = CountVectorizer(analyzer=lambda x: x)
rec = bag.fit_transform(df['ingredients'])
counts = pd.DataFrame({'ingredient': bag.get_feature_names_out(),'count': rec.toarray().sum(axis=0)})

figure(figsize=(10, 9))
plt.axis('off')
plt.imshow(WordCloud(width=400, height=300).generate_from_frequencies(dict(zip(counts['ingredient'].to_list(), counts['count'].to_list()))))

df['cuisine'].value_counts()

new = []
for s in df['ingredients']:
    s = ' '.join(s)
    new.append(s)

df['ing'] = new

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('word_tokenize')

import re
l=[]
for s in df['ing']:

    #Remove punctuations
    s=re.sub(r'[^\w\s]','',s)

    #Remove Digits
    s=re.sub(r"(\d)", "", s)

    #Remove content inside paranthesis
    s=re.sub(r'\([^)]*\)', '', s)

    #Remove Brand Name
    s=re.sub(u'\w*\u2122', '', s)

    #Convert to lowercase
    s=s.lower()

    #Remove Stop Words
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(s)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    filtered_sentence = []
    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)
    s=' '.join(filtered_sentence)

    #Remove low-content adjectives


    #Porter Stemmer Algorithm
    words = word_tokenize(s)
    word_ps=[]
    for w in words:
       word_ps.append(ps.stem(w))
    s=' '.join(word_ps)

    l.append(s)
df['ing_mod']=l
print(df.head(10))

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['ing_mod'])

print(X)

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(df['cuisine'])
df['cuisine']=le.transform(df['cuisine'])

print(df['cuisine'].value_counts())

cus_lis = [0,15,10,8,4,1,14,19,12,17,6,11,18,2,5,3,7,16,13]

cuisine_map={'0':'brazilian', '1':'british', '2':'cajun_creole', '3':'chinese', '4':'filipino', '5':'french', '6':'greek', '7':'indian', '8':'irish', '9':'italian', '10':'jamaican', '11':'japanese', '12':'korean', '13':'mexican', '14':'moroccan', '15':'russian', '16':'southern_us', '17':'spanish', '18':'thai', '19':'vietnamese'}

Y=[]
Y = df['cuisine']

import numpy as np
from sklearn.impute import SimpleImputer as Imputer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 100)
for K in range(25):
 K_value = K+1
 neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')
 neigh.fit(X_train, y_train)
 y_pred = neigh.predict(X_test)
 print("Accuracy is ", accuracy_score(y_test,y_pred)*100,"% for K-Value:",K_value)
 print("F1 Score:" , f1_score(y_test, y_pred, average='macro'))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
RFclf = RandomForestClassifier(random_state=0)
RFclf.fit(X_train, y_train)
y_pred=RFclf.predict(X_test)
print(accuracy_score(y_test,y_pred)*100)
print(f1_score(y_test, y_pred, average='macro'))

from sklearn.naive_bayes import MultinomialNB
gnb = MultinomialNB(force_alpha=True)
gnb.fit(X_train, y_train)
y_pred=gnb.predict(X_test)
print(accuracy_score(y_test,y_pred)*100)
print(f1_score(y_test, y_pred, average='macro'))

from sklearn import svm, datasets
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
parameter_candidates = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]
clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)
clf.fit(X_train, y_train)
print('Best score for data1:', clf.best_score_)
print('Best C:',clf.best_estimator_.C)
print('Best Kernel:',clf.best_estimator_.kernel)
print('Best Gamma:',clf.best_estimator_.gamma)

from sklearn import svm
lin_clf = svm.LinearSVC(C=1)
lin_clf.fit(X_train, y_train)
y_pred=lin_clf.predict(X_test)
print(accuracy_score(y_test,y_pred)*100)

lin_clf = svm.LinearSVC(C=1.0, multi_class='crammer_singer')
lin_clf.fit(X_train, y_train)
y_pred=lin_clf.predict(X_test)
print(accuracy_score(y_test,y_pred)*100)

temp_df = df.copy()
from sklearn import svm
from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit
import matplotlib.pyplot as plt
f = 20
# for i in range(2):
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(temp_df['ing_mod'])
Y=[]
Y = temp_df['cuisine']
# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 100)
lin_clf = svm.LinearSVC(C=1)

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))
common_params = {
  "X": X,
  "y": Y,
  "train_sizes": np.linspace(0.1, 1.0, 5),
  "cv": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),
  "score_type": "both",
  "n_jobs": 4,
  "line_kw": {"marker": "o"},
  "std_display_style": "fill_between",
  "score_name": "Accuracy",
}

for ax_idx, estimator in enumerate([lin_clf]):
    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax)
    handles, label = ax.get_legend_handles_labels()
    ax.legend(handles[:2], ["Training Score", "Test Score"])
    ax.set_title(f"Learning Curve for {estimator.__class__.__name__}")

  # for j in cus_lis[:18]:
  #   temp_df = temp_df[temp_df.cuisine!=i]
  # print(len(temp_df))
  # # print()
  # # print("Removing",cuisine_map[str(i)],"recipies")
  # f -= 1

temp_df = df.copy()
from sklearn import svm
from sklearn.metrics import f1_score
f = 20
for i in cus_lis:
  vectorizer = TfidfVectorizer()
  X = vectorizer.fit_transform(temp_df['ing_mod'])
  Y=[]
  Y = temp_df['cuisine']
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 100)
  lin_clf = svm.LinearSVC(C=1)
  lin_clf.fit(X_train, y_train)
  y_pred=lin_clf.predict(X_test)
  print("accuracy score for top",f,"cuisines" ,accuracy_score(y_test,y_pred)*100)
  print(f1_score(y_test, y_pred, average='macro'))

  temp_df = temp_df[temp_df.cuisine!=i]
  # print()
  # print("Removing",cuisine_map[str(i)],"recipies")
  f -= 1

"""# Brute Force Thingie starts here.......

"""

all_ing = []
total_ing = 0
x = 0
for i in dict_train:
  # if i["cuisine"] == 'mexican' or i["cuisine"] == 'italian':
    all_ing.extend(i["ingredients"])
    all_ing = list(set(all_ing))
    total_ing += len(i["ingredients"])
    x+=1

print(len(all_ing))
print("average ingredients =", total_ing/x)

# temp_df = df.copy()
# from sklearn import svm
# f = 18
# for i in cus_lis[:18]:
#   vectorizer = TfidfVectorizer()
#   X = vectorizer.fit_transform(temp_df['ing_mod'])
#   Y=[]
#   Y = temp_df['cuisine']
#   temp_df = temp_df[temp_df.cuisine!=i]
#   # print()
#   # print("Removing",cuisine_map[str(i)],"recipies")
#   f -= 1

# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 100)
# model_top2 = svm.LinearSVC(C=1)
# model_top2.fit(X_train, y_train)
# y_pred=model_top2.predict(X_test)
# print(accuracy_score(y_test,y_pred)*100)

# print(temp_df['cuisine'].value_counts())

test = dict_train[909]
print(test)

test_dict = []
for i in range(len(test["ingredients"])):
  for j in all_ing:
    temp_ing = test["ingredients"].copy()
    if j not in temp_ing:
      temp_ing[i] = j
      test_dict.append(temp_ing)

print(test_dict[0:5])
print(len(test_dict))

new_test = []
for s in test_dict:
    s = ' '.join(s)
    new_test.append(s)

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('word_tokenize')

import re
l=[]
for s in new_test:

    #Remove punctuations
    s=re.sub(r'[^\w\s]','',s)

    #Remove Digits
    s=re.sub(r"(\d)", "", s)

    #Remove content inside paranthesis
    s=re.sub(r'\([^)]*\)', '', s)

    #Remove Brand Name
    s=re.sub(u'\w*\u2122', '', s)

    #Convert to lowercase
    s=s.lower()

    #Remove Stop Words
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(s)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    filtered_sentence = []
    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)
    s=' '.join(filtered_sentence)

    #Remove low-content adjectives


    #Porter Stemmer Algorithm
    words = word_tokenize(s)
    word_ps=[]
    for w in words:
       word_ps.append(ps.stem(w))
    s=' '.join(word_ps)

    l.append(s)
test_final=l
print(test_final[0:10])

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
test_vec = vectorizer.fit_transform(test_final)

print(test_vec)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

new_pred_pro = lin_clf.decision_function(test_vec)

pred_probability = []
for eachArr in new_pred_pro:
    pred_probability.append(softmax(eachArr))

fin = -1
for i in range(len(pred_probability)):
  m = max(pred_probability[i])
  if list(pred_probability[i]).index(m) == 9 and m>pred_probability[fin][9]:
    fin = i

print(test)
print(test_dict[fin])
print(pred_probability[fin][9])
